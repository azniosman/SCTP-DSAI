{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace7e04a",
   "metadata": {},
   "source": [
    "# Unsupervised Learning using Scikit Learn - Machine Learning with Python\n",
    "\n",
    "Includes adaptions from jovian.ai\n",
    "\n",
    "![](https://i.imgur.com/eyfi64y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3853208",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The following topics are covered in this tutorial:\n",
    "\n",
    "- Overview of unsupervised learning algorithms in Scikit-learn\n",
    "- Anomaly Detection algorithms: Local Outlier Factor (LOF)\n",
    "- Clustering algorithms: K Means, DBScan, Hierarchical clustering etc.\n",
    "- Dimensionality reduction (PCA) and manifold learning (t-SNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e540bc9",
   "metadata": {},
   "source": [
    "## Introduction to Unsupervised Learning\n",
    "\n",
    "Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here's how unsupervised learning fits into the landscape of machine learning algorithms([source](https://medium.datadriveninvestor.com/machine-learning-in-10-minutes-354d83e5922e)):\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/842/1*tlQwBmbL6RkuuFq8OPJofw.png\" width=\"640\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f48eb",
   "metadata": {},
   "source": [
    "Here are the topics in machine learning that we're studying in this course ([source](https://vas3k.com/blog/machine_learning/)): \n",
    "\n",
    "<img src=\"https://i.imgur.com/K3IOQyh.png\" width=\"640\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddfaaa",
   "metadata": {},
   "source": [
    "## Outlier and Anomaly Detection\n",
    "\n",
    "**Outliers** are data points that deviate significantly from the majority of the data in a dataset. They are unusual values that are not in line with the general distribution of the data. Outliers can be the result of variability in the measurement, experimental errors or data entry errors. They could also represent a novel or rare event.\n",
    "\n",
    "**Anomalies** are patterns in data that do not conform to a well-defined notion of normal behavior. Anomalies are similar to outliers but are often considered in the context of temporal or complex data. Anomaly detection is particularly relevant in sequential data or in situations where the data is generated by complex systems, such as cyber-security, fraud detection, or health monitoring.\n",
    "\n",
    "Detecting outliers and anomalies is important for several reasons:\n",
    "\n",
    "1. **Data Cleaning**: Outliers can arise due to errors in data collection or entry, and identifying these can help clean the data and improve the quality of the dataset.\n",
    "\n",
    "2. **Robustness**: Machine learning models can be sensitive to outliers. Detecting and handling outliers can make models more robust and improve their generalization.\n",
    "\n",
    "3. **Insights**: Outliers and anomalies can sometimes be the most interesting part of the data. For example, in fraud detection, the fraudulent activities are the anomalies that we want to detect.\n",
    "\n",
    "4. **Safety**: In critical systems, such as healthcare monitoring or aviation, anomaly detection can provide early warnings about potentially dangerous conditions.\n",
    "\n",
    "Several popular methods for outlier and anomaly detection include Z-Score, Local Outlier Factor (LOF), Mahalanobis Distance and One-Class SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185dc3f8",
   "metadata": {},
   "source": [
    "### Z-Score Method\n",
    "\n",
    "The Z-Score, also known as the standard score, indicates how many standard deviations an element is from the mean. A Z-Score can be calculated for each data point to determine if it's an outlier.\n",
    "\n",
    "#### Steps to Calculate Z-Score:\n",
    "\n",
    "1. Calculate the mean (μ) and standard deviation (σ) of the feature / variable.\n",
    "2. For each data point (x), calculate the Z-Score\n",
    "3. Determine a threshold Z-Score (e.g., 3 or -3). Data points with a Z-Score beyond this threshold are considered outliers.\n",
    "\n",
    "#### Z-Score Formula\n",
    "\n",
    "$$ Z = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "\n",
    "where:\n",
    "- $X$ is the data point.\n",
    "- $\\mu$ is the mean of the dataset.\n",
    "- $\\sigma$ is the standard deviation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80112493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009158f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = np.array([10, 12, 12, 13, 12, 11, 14, 13, 15, 10, 10, 100, 12, 14, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2046952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "std_dev = np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-Scores\n",
    "z_scores = (data - mean) / std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold and detect outliers\n",
    "threshold = 3\n",
    "outliers = np.where(np.abs(z_scores) > threshold)\n",
    "\n",
    "print(f\"Outlier indices: {outliers[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5893ad7",
   "metadata": {},
   "source": [
    "### Local Outlier Factor (LOF)\n",
    "\n",
    "LOF is an algorithm that detects outliers by measuring the local deviation of a given data point concerning its neighbors. It considers the density of a data point's neighborhood and compares it to the density of its neighbors' neighborhoods.\n",
    "\n",
    "![lof](../assets/lof.png)\n",
    "\n",
    "#### Steps to Use LOF:\n",
    "\n",
    "1. Choose the number of neighbors (k).\n",
    "2. Calculate the reachability distance and local reachability density for each point.\n",
    "3. Calculate the LOF score for each point. A score significantly larger than 1 indicates an outlier.\n",
    "\n",
    "#### LOF Formula\n",
    "\n",
    "$$ LOF_k(A) = \\frac{\\sum_{B \\in N_k(A)} \\frac{lrd(B)}{lrd(A)}}{|N_k(A)|} $$\n",
    "\n",
    "where:\n",
    "- $N_k(A)$ is the set of $k$ nearest neighbors of $A$.\n",
    "- $lrd$ is the local reachability density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5973b41",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance\n",
    "\n",
    "Mahalanobis Distance is a multivariate measure of the distance between a point and a distribution. It's an effective way to detect outliers, especially when dealing with multidimensional data.\n",
    "\n",
    "#### Steps to Calculate Mahalanobis Distance:\n",
    "\n",
    "1. Calculate the covariance matrix of the dataset.\n",
    "2. Calculate the inverse of the covariance matrix.\n",
    "3. For each data point (x), calculate its Mahalanobis Distance using the formula: D² = (x - μ)ᵀ * S⁻¹ * (x - μ), where S⁻¹ is the inverse covariance matrix.\n",
    "\n",
    "#### Mahalanobis Distance Formula\n",
    "\n",
    "$$ D^2 = (x - \\mu)^T S^{-1} (x - \\mu) $$\n",
    "\n",
    "where:\n",
    "- $x$ is the data point.\n",
    "- $\\mu$ is the mean of the dataset.\n",
    "- $S$ is the covariance matrix of the dataset.\n",
    "- $S^{-1}$ is the inverse of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "X = [[10, 10], [12, 12], [11, 11], [14, 14], [100, 100], [14, 14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=2)\n",
    "outlier_labels = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 indicates an outlier\n",
    "print(f\"Outlier labels: {outlier_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using Mahalanobis distance\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45314c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10, 10], [12, 12], [11, 11], [14, 14], [100, 100], [14, 14]])\n",
    "# Calculate mean and covariance matrix\n",
    "\n",
    "mean = np.mean(X, axis=0)\n",
    "cov_matrix = np.cov(X.T)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mahalanobis Distance for each data point\n",
    "mahalanobis_distances = np.array([np.dot(np.dot((x - mean), inv_cov_matrix), (x - mean).T) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a critical value from the chi-squared distribution\n",
    "critical_value = chi2.ppf((1-0.1), df=2)  # df is the number of dimensions\n",
    "outliers = np.where(mahalanobis_distances > critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Outlier indices: {outliers[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7781c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2aa02",
   "metadata": {},
   "source": [
    "### One-Class SVM\n",
    "\n",
    "One-Class SVM is an extension of the SVM algorithm that has been adapted for anomaly detection. The goal of One-Class SVM is to find a function that captures the region where the majority of the data points reside. Data points that do not fit within this region are considered anomalies.\n",
    "\n",
    "### How One-Class SVM Works\n",
    "\n",
    "One-Class SVM works by mapping the input data points into a high-dimensional feature space using a kernel function. It then tries to separate the data from the origin with maximum margin. This results in a decision boundary around the normal data points. Points that fall outside this boundary are deemed anomalies.\n",
    "\n",
    "### Parameters of One-Class SVM\n",
    "\n",
    "- `nu`: An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. It must be between 0 and 1. Essentially, it corresponds to the proportion of outliers you expect in your data.\n",
    "- `kernel`: Specifies the kernel type to be used in the algorithm. It can be `linear`, `poly`, `rbf`, `sigmoid`, `precomputed`, or a callable.\n",
    "- `gamma`: Kernel coefficient for `rbf`, `poly`, and `sigmoid`. It controls the influence of individual training samples - this affects the smoothness of the model. A low value of gamma will give a more flexible decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0994d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_normal = np.random.randn(100, 2)\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X = np.vstack([X_normal, X_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c844847",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39077bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"One-Class SVM Anomaly Detection\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=plt.cm.Paired, edgecolors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6635a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037a0d2",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Manifold Learning\n",
    "\n",
    "In machine learning problems, we often encounter datasets with a very large number of dimensions (features or columns). Dimensionality reduction techniques are used to reduce the number of dimensions or features within the data to a manageable or convenient number. \n",
    "\n",
    "\n",
    "Applications of dimensionality reduction:\n",
    "\n",
    "* Reducing size of data without loss of information\n",
    "* Training machine learning models efficiently\n",
    "* Visualizing high-dimensional data in 2/3 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea83267",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Principal component is a dimensionality reduction technique that uses linear projections of data to reduce their dimensions, while attempting to maximize the variance of data in the projection. Watch this video to learn how PCA works: https://www.youtube.com/watch?v=FgakZw6K1QQ&t=212s\n",
    "\n",
    "#### How PCA Works\n",
    "\n",
    "1. Standardize the data.\n",
    "2. Compute the covariance matrix of the data.\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Sort the eigenvectors by decreasing eigenvalues and choose the first k eigenvectors. This forms a new matrix.\n",
    "5. Transform the original dataset using this eigenvector matrix to obtain the new k-dimensional feature subspace.\n",
    "\n",
    "#### PCA Formula\n",
    "\n",
    "PCA involves the eigen decomposition of the data covariance matrix or singular value decomposition of the data matrix, usually after mean centering the data for each attribute.\n",
    "\n",
    "The covariance matrix of a data matrix $X$ is given by:\n",
    "\n",
    "$$ S = \\frac{1}{n-1} X^T X $$\n",
    "\n",
    "where $n$ is the number of data points.\n",
    "\n",
    "The eigen decomposition of $S$ is then computed to obtain the principal components. The eigenvectors of $S$ correspond to the directions of maximum variance, ordered by the corresponding eigenvalues in descending order.\n",
    "\n",
    "#### PCA example\n",
    "\n",
    "Here's a visual example of PCA to reduce 2D data to 1D:\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZJ7utlo.png\" width=\"480\">\n",
    "\n",
    "Let's apply Principal Component Analysis to the Iris dataset.\n",
    "\n",
    "<img src=\"../assets/iris.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = sns.load_dataset('iris')\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbba57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f77bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "?PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d47c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(iris_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1120b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = pca.transform(iris_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b747de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5a562",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "As you can see, the PCA algorithm has done a very good job of separating different species of flowers using just 2 measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1087a",
   "metadata": {},
   "source": [
    "Learn more about Principal Component Analysis here: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805f2f7",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. Scikit-learn provides many algorithms for manifold learning: https://scikit-learn.org/stable/modules/manifold.html . A commonly-used manifold learning technique is t-Distributed Stochastic Neighbor Embedding or t-SNE, used to visualize high dimensional data in one, two or three dimensions. \n",
    "\n",
    "#### How t-SNE Works\n",
    "\n",
    "1. Measure the similarity between pairs of instances in the high-dimensional space and in the low-dimensional space.\n",
    "2. Use these similarities to create a probability distribution that represents similarities between instances.\n",
    "3. Minimize the divergence between these two distributions with respect to the positions of the points in the map.\n",
    "\n",
    "#### t-SNE Formula\n",
    "\n",
    "t-SNE first computes probabilities $p_{ij}$ that are proportional to the similarity of objects $i$ and $j$ in the high-dimensional space, using a Gaussian distribution centered at object $i$:\n",
    "\n",
    "$$ p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)} $$\n",
    "\n",
    "$$ p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n} $$\n",
    "\n",
    "In the low-dimensional space, t-SNE uses a Student-t distribution to compute similar probabilities $q_{ij}$:\n",
    "\n",
    "$$ q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}} $$\n",
    "\n",
    "t-SNE then minimizes the Kullback-Leibler divergence between the distributions $P$ and $Q$ to find a good representation of the data in the low-dimensional space:\n",
    "\n",
    "$$ KL(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}} $$\n",
    "\n",
    "#### t-SNE Example\n",
    "\n",
    "Here's a visual representation of t-SNE applied to visualize 2 dimensional data in 1 dimension:\n",
    "\n",
    "<img src=\"https://i.imgur.com/rVMAaix.png\" width=\"360\">\n",
    "\n",
    "\n",
    "Here's a video explaning how t-SNE works: https://www.youtube.com/watch?v=NEaUSP4YerM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "?TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3312f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = tsne.fit_transform(iris_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb949686",
   "metadata": {},
   "source": [
    "As you can see, the flowers from the same species are clustered very closely together. The relative distance between the species is also conveyed by the gaps between the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba47021",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Use t-SNE to visualize the [MNIST handwritten digits dataset](https://www.kaggle.com/oddrationale/mnist-in-csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70066ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize MNIST data using t-SNE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST test data\n",
    "print(\"Loading MNIST data...\")\n",
    "df = pd.read_csv(\"./mnist_test.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "print(f\"First few labels: {df.iloc[:5, 0].values}\")  # Show first 5 labels\n",
    "print(f\"First rows: {df.head(5)}\")  # Show first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (pixel values) and labels\n",
    "print(\"Preprocessing data...\")\n",
    "y = df[\"label\"].values\n",
    "X = df.drop(\"label\", axis=1).values\n",
    "\n",
    "# Plot MNIST digits\n",
    "def plot_digits(images, labels, num_samples=10, cols=5):\n",
    "    \"\"\"\n",
    "    Plot MNIST digits in a grid\n",
    "    \n",
    "    Parameters:\n",
    "    images: array of flattened 784-pixel images\n",
    "    labels: array of corresponding labels\n",
    "    num_samples: number of samples to plot\n",
    "    cols: number of columns in the grid\n",
    "    \"\"\"\n",
    "    rows = int(np.ceil(num_samples / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 8))\n",
    "    axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Reshape the flattened image to 28x28\n",
    "        image = images[i].reshape(28, 28)\n",
    "        \n",
    "        # Plot the image\n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {labels[i]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the first 10 digits from the dataset\n",
    "print(\"\\nPlotting first 20 digits...\")\n",
    "plot_digits(X, y, num_samples=20)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06774ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Take a sample of the data for faster computation\n",
    "sample_size = 2000  # Using a larger sample for better visualization\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))[:sample_size]\n",
    "X_sample = X[indices]\n",
    "y_sample = y[indices]\n",
    "\n",
    "# Standardize the data\n",
    "print(\"Standardizing data...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_sample)\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"Running t-SNE (this may take a few minutes)...\")\n",
    "tsne = TSNE(\n",
    "##########!!!!! ADD YOUR TSNE PARAMETERS HERE !!!!!!##########    \n",
    ")\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 12))\n",
    "scatter = plt.scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=y_sample,\n",
    "    cmap=\"tab10\",\n",
    "    alpha=0.7,\n",
    "    s=15,\n",
    "    edgecolors=\"none\"\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Digit\", ticks=range(10))\n",
    "plt.title(\"t-SNE Visualization of MNIST Test Data (n={})\".format(sample_size), fontsize=16)\n",
    "plt.xlabel(\"t-SNE 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fdb28",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "Clustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups ([Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ab11e",
   "metadata": {},
   "source": [
    "Here is a visual representation of clustering:\n",
    "\n",
    "<img src=\"https://i.imgur.com/VXPgw6H.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1a32c",
   "metadata": {},
   "source": [
    "Here are some real-world applications of clustering:\n",
    "\n",
    "* Customer segmentation \n",
    "* Product recommendation\n",
    "* Feature engineering\n",
    "* Anomaly/fraud detection\n",
    "* Taxonomy creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001058e",
   "metadata": {},
   "source": [
    "We'll use the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) to study some of the clustering algorithms available in `scikit-learn`. It contains various measurements for 150 flowers belonging to 3 different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd26680",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c6674",
   "metadata": {},
   "source": [
    "We'll attempt to cluster observations using numeric columns in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197020d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1125d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X = iris_df[numeric_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f34c8c",
   "metadata": {},
   "source": [
    "### K Means Clustering\n",
    "\n",
    "The K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*rw8IUza1dbffBhiA4i0GNQ.png\" width=\"640\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a68e66",
   "metadata": {},
   "source": [
    "Here's how the K-means algorithm works:\n",
    "\n",
    "1. Pick K random objects as the initial cluster centers.\n",
    "2. Classify each object into the cluster whose center is closest to the point.\n",
    "3. For each cluster of classified objects, compute the centroid (mean).\n",
    "4. Now reclassify each object using the centroids as cluster centers.\n",
    "5. Calculate the total variance of the clusters (this is the measure of goodness).\n",
    "6. Repeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n",
    "\n",
    "The goal of K-Means is to minimize the within-cluster sum of squares (WCSS), which is defined as:\n",
    "\n",
    "$$ WCSS = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2 $$\n",
    "\n",
    "where $C_i$ is the set of points in cluster $i$ and $\\mu_i$ is the centroid of cluster $i$.\n",
    "\n",
    "![kmeans](../assets/kmeans.png)\n",
    "\n",
    "Here's a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f109a70",
   "metadata": {},
   "source": [
    "Let's apply K-means clustering to the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b8fda",
   "metadata": {},
   "source": [
    "We can check the cluster centers for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a5079",
   "metadata": {},
   "source": [
    "We can now classify points using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e34a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0afdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds);\n",
    "centers_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\n",
    "plt.plot(centers_x, centers_y, 'xb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec534f1",
   "metadata": {},
   "source": [
    "As you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the \"species\" column as an input to `KMeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc30ce7",
   "metadata": {},
   "source": [
    "We can check the \"goodness\" of the fit by looking at `model.inertia_`, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e66762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359e639",
   "metadata": {},
   "source": [
    "Let's try creating 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=6, random_state=42).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c851bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e7962",
   "metadata": {},
   "source": [
    "In most real-world scenarios, there's no predetermined number of clusters. In such a case, you can create a plot of \"No. of clusters\" vs \"Inertia\" to pick the right number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a08068",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = range(2,8)\n",
    "inertias = []\n",
    "\n",
    "for n_clusters in options:\n",
    "    model = KMeans(n_clusters, random_state=42).fit(X)\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.title(\"No. of clusters vs. Inertia\")\n",
    "plt.plot(options, inertias, '-o')\n",
    "plt.xlabel('No. of clusters (K)')\n",
    "plt.ylabel('Inertia');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be131b53",
   "metadata": {},
   "source": [
    "The chart creates an \"elbow\" plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17668d",
   "metadata": {},
   "source": [
    "**Silhouette Score Analysis**\n",
    "\n",
    "The silhouette score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The score ranges from -1 to 1, where:\n",
    "- A score close to 1 indicates the point is well-matched to its own cluster and poorly matched to neighboring clusters\n",
    "- A score of 0 indicates the point is on or very close to the decision boundary between two neighboring clusters\n",
    "- A negative score indicates that the point might have been assigned to the wrong cluster\n",
    "\n",
    "We'll calculate the silhouette score for different numbers of clusters to find the optimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63314e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette scores for different numbers of clusters\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in options:\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
    "    score = silhouette_score(X, model.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"Silhouette score for {n_clusters} clusters: {score:.4f}\")\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(options, silhouette_scores, '-o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "\n",
    "# Visualize the clusters for k=3\n",
    "plt.subplot(1, 2, 2)\n",
    "for k in [3]:\n",
    "    model = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "    plt.scatter(X['sepal_length'], X['petal_length'], c=model.labels_, \n",
    "               alpha=0.6, label=f'k={k}')\n",
    "    centers = model.cluster_centers_\n",
    "    # Plot cluster centers with red X marks\n",
    "    plt.scatter(centers[:, 0], centers[:, 2], c='red', marker='x', s=200, label='Cluster Centers')\n",
    "\n",
    "plt.title('Cluster Comparison (k=3)')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Petal Length')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7169677",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae63e5",
   "metadata": {},
   "source": [
    "#### Understanding the Silhouette Score Results\n",
    "\n",
    "The silhouette score is highest for 2 clusters rather than 3, even though we know there are 3 iris species. Here's why this happens:\n",
    "\n",
    "**Cluster Separation**: The Iris species are not perfectly separated in the feature space. \n",
    "   - Two of the species (versicolor and virginica) have significant overlap in their measurements\n",
    "   - The setosa species is quite distinct from the other two\n",
    "\n",
    "**What This Means**:\n",
    "   - The silhouette score is telling us that the natural separation in the data is stronger between setosa vs. others, rather than between all three species\n",
    "   - This is actually correct from a clustering perspective - two of the species are indeed very similar to each other\n",
    "   - The \"correct\" number of clusters depends on what you're trying to achieve with your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368963a",
   "metadata": {},
   "source": [
    "**Mini Batch K Means**: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a79fb9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "> **EXERCISE**: Perform clustering on the [Mall customers dataset](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python) on Kaggle. Study the segments carefully and report your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_df = pd.read_csv(\"./Mall_Customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee05a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e93f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad98300",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5548a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=mall_df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Spending Score (1-100)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_model = KMeans(n_clusters=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad149f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_numeric_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = mall_df[mall_numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66676727",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_model.fit(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964eaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3594c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "options = range(2,11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in options:\n",
    "    # Fit KMeans\n",
    "    model = KMeans(n_clusters, random_state=42).fit(M)\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "    # Calculate Silhouette Score\n",
    "    score = silhouette_score(M, model.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f'Clusters: {n_clusters}, Silhouette Score: {score:.3f}')\n",
    "\n",
    "# Plot Inertia\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(options, inertias, '-o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(options, silhouette_scores, '-o')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae56719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KMeans with optimal number of clusters\n",
    "n_clusters = 6\n",
    "model_kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(M)\n",
    "\n",
    "# Get predictions\n",
    "pred_m = model_kmeans.predict(M)\n",
    "\n",
    "# Print cluster distribution\n",
    "print(\"Cluster distribution:\")\n",
    "unique, counts = np.unique(pred_m, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"Cluster {cluster}: {count} samples\")\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=M, x='Annual Income (k$)', y='Spending Score (1-100)', \n",
    "                hue=pred_m, palette='viridis', legend='full')\n",
    "plt.title(f'Customer Segments (K={n_clusters})')\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "# Calculate and print silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(M, pred_m)\n",
    "print(f'\\nSilhouette Score for {n_clusters} clusters: {silhouette_avg:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f233ae3",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "Density-based spatial clustering of applications with noise (DBSCAN) uses the density of points in a region to form clusters. It has two main parameters: \"epsilon\" and \"min samples\" using which it classifies each point as a core point, reachable point or noise point (outlier).\n",
    "\n",
    "#### Steps for DBSCAN Clustering:\n",
    "\n",
    "![dbscan1](../assets/dbscan1.png)\n",
    "\n",
    "1. **Choose Parameters**:\n",
    "   - Select values for two parameters: `eps` (epsilon) and `MinPts` (minimum points).\n",
    "     - `eps`: The radius of the neighborhood around a point.\n",
    "     - `MinPts`: The minimum number of points required to form a dense region, i.e., for a point to be considered a core point.\n",
    "\n",
    "2. **Identify Core Points, Border Points, and Noise**:\n",
    "   - For each point in the dataset, count how many points fall within the `eps` neighborhood (including the point itself).\n",
    "   - A point is labeled as a core point if at least `MinPts` points fall within its `eps` neighborhood.\n",
    "   - A point is labeled as a border point if fewer than `MinPts` points fall within its `eps` neighborhood, but it is in the neighborhood of a core point.\n",
    "   - A point is labeled as noise if it is neither a core point nor a border point.\n",
    "\n",
    "3. **Form Clusters**:\n",
    "   - For each core point, if it is not already assigned to a cluster, create a new cluster, and recursively add all points that are directly reachable from it in the `eps` neighborhood to the cluster.\n",
    "   - Merge clusters if a core point is reachable from points in different clusters.\n",
    "\n",
    "4. **Assign Border Points**:\n",
    "   - Assign each border point to one of the clusters of its associated core points.\n",
    "\n",
    "5. **Handle Noise**:\n",
    "   - Points classified as noise are not assigned to any cluster. They can either be discarded or processed separately, depending on the context of the analysis.\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "- Does not require specifying the number of clusters beforehand.\n",
    "- Can find arbitrarily shaped clusters.\n",
    "- Robust to outliers.\n",
    "- Has a notion of noise.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "- Can struggle with clusters of varying densities.\n",
    "- The quality of clustering depends heavily on the choice of parameters `eps` and `MinPts`.\n",
    "- If the data and dimensionality are very high, it may be computationally expensive to compute all the neighborhoods.\n",
    "\n",
    "Here's a video explaining how the DBSCAN algorithm works: https://www.youtube.com/watch?v=C3r7tGRe2eI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0910656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44244049",
   "metadata": {},
   "outputs": [],
   "source": [
    "?DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=0.6, min_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4656f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43224fb1",
   "metadata": {},
   "source": [
    "In DBSCAN, there's no prediction step. It directly assigns labels to all the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c930e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=model.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67a083",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Try changing the values of `eps` and `min_samples` and observe how the number of clusters the classification changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b9823",
   "metadata": {},
   "source": [
    "Here's how the results of DBSCAN and K Means differ:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1339/0*xu3GYMsWu9QiKNOo.png\" width=\"640\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bdbf8",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering, as the name suggests, creates a hierarchy or a tree of clusters.\n",
    "\n",
    "<img src=\"https://dashee87.github.io/images/hierarch.gif\" width=\"640\">\n",
    "\n",
    "While there are several approaches to hierarchical clustering, the most common approach works as follows:\n",
    "\n",
    "#### Steps for Agglomerative Hierarchical Clustering:\n",
    "\n",
    "1. **Start by Assigning Each Data Point to Its Own Cluster**:\n",
    "   - If you have `N` data points, you start with `N` clusters, each containing just one data point.\n",
    "\n",
    "2. **Compute the Proximity Matrix**:\n",
    "   - Calculate the distance between each pair of clusters. There are various distance metrics that can be used, such as Euclidean, Manhattan, or Cosine distance. The choice of distance metric depends on the nature of the data.\n",
    "\n",
    "3. **Merge the Closest Pair of Clusters**:\n",
    "   - Find the two clusters that are closest to each other and combine them into a single cluster. As a result, the number of clusters decreases by one.\n",
    "\n",
    "4. **Update the Proximity Matrix**:\n",
    "   - After merging two clusters, the proximity matrix needs to be updated to reflect the distances between the new cluster and the original clusters. The method for updating the matrix depends on the linkage criterion:\n",
    "     - **Single Linkage (MIN)**: Distance between two clusters is the minimum distance between any single data point in the first cluster and any single data point in the second cluster.\n",
    "     \n",
    "       $$ d(A,B) = \\min_{a \\in A, b \\in B} d(a, b) $$\n",
    "       \n",
    "     - **Complete Linkage (MAX)**: Distance between two clusters is the maximum distance between any single data point in the first cluster and any single data point in the second cluster.\n",
    "     \n",
    "       $$ d(A,B) = \\max_{a \\in A, b \\in B} d(a, b) $$\n",
    "       \n",
    "     - **Average Linkage**: Distance between two clusters is the average distance between each data point in the first cluster and every data point in the second cluster.\n",
    "     \n",
    "       $$ d(A,B) = \\frac{1}{|A| \\cdot |B|} \\sum_{a \\in A} \\sum_{b \\in B} d(a, b) $$\n",
    "       \n",
    "     - **Ward's Method**: Distance between two clusters is the increase in the total within-cluster variance after merging them, which is equivalent to the squared Euclidean distance between cluster means.\n",
    "     \n",
    "       $$ d(A,B) = \\sum_{i=1}^{n} (x_{iA} - x_{iB})^2 $$\n",
    "       \n",
    "     where $x_{iA}$ and $x_{iB}$ are the means of clusters $A$ and $B$ for variable $i$, and $n$ is the number of variables.\n",
    "\n",
    "5. **Repeat Steps 3 and 4**:\n",
    "   - Continue merging the closest pair of clusters and updating the proximity matrix until all data points are clustered into a single cluster that contains all observations.\n",
    "\n",
    "6. **Construct a Dendrogram**:\n",
    "   - A dendrogram is a tree-like diagram that records the sequences of merges or splits. It provides a visual summary of the clustering process, showing the order of cluster combinations and the distance at which each merge occurred.\n",
    "\n",
    "Hierarchical clustering doesn't require specifying the number of clusters beforehand, but it can be determined by cutting the dendrogram at a desired level. The choice of where to cut the dendrogram depends on the problem context or a predetermined threshold distance.\n",
    "\n",
    "![hierarchical](../assets/hierarchical.png)\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "1. **No Need to Specify Number of Clusters**: You do not need to specify the number of clusters beforehand. The number of clusters can be determined by cutting the dendrogram at the desired level.\n",
    "2. **Dendrogram**: Provides a visual representation of the clustering process, which can be informative.\n",
    "3. **Flexibility**: Can create clusters of various shapes and sizes, not limited to spherical clusters like K-Means.\n",
    "4. **Easy to Merge/Split Clusters**: The hierarchical nature allows for easy merging or splitting of clusters based on the hierarchy.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "1. **Computational Complexity**: The time complexity for the standard algorithm is typically O(n^2 log n) or O(n^3), which can be prohibitive for large datasets.\n",
    "2. **Irreversible**: Once a decision is made to combine two clusters, it cannot be undone without restarting the entire process.\n",
    "3. **Sensitivity to Noise and Outliers**: Similar to K-Means, hierarchical clustering can be sensitive to noise and outliers, which can lead to misinterpretations of the data structure.\n",
    "4. **Choice of Linkage Criteria**: The results can be significantly different based on the choice of linkage criteria (single, complete, average, etc.), and there is no objective way to choose the best method.\n",
    "\n",
    "Watch this video for a visual explanation of hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1d86a",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Implement hierarchical clustering for the Iris dataset using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9138602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # True labels (for comparison)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(X, 'ward')  # Using Ward's method for linkage\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linked, orientation='top', labels=y, distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Iris Dataset)')\n",
    "plt.xlabel('Sample index or (cluster size)')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Cut the dendrogram to get cluster labels\n",
    "num_clusters = 3  # We know there are 3 species in Iris dataset\n",
    "cluster_labels = fcluster(linked, t=num_clusters, criterion='maxclust') - 1  # Convert to 0-based index\n",
    "\n",
    "# Visualize the clusters using the first two features\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', alpha=0.8)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('Hierarchical Clustering Results (Iris Dataset)')\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "# Print cluster sizes\n",
    "print(\"Cluster sizes:\", np.bincount(cluster_labels))\n",
    "\n",
    "# Calculate silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0d9f6",
   "metadata": {},
   "source": [
    "## Summary and References\n",
    "\n",
    "<img src=\"https://i.imgur.com/K3IOQyh.png\" width=\"640\">\n",
    "\n",
    "The following topics were covered in this tutorial:\n",
    "\n",
    "- Overview of unsupervised learning algorithms in Scikit-learn\n",
    "- Clustering algorithms: K Means, DBScan, Hierarchical clustering etc.\n",
    "- Dimensionality reduction (PCA) and manifold learning (t-SNE)\n",
    "\n",
    "\n",
    "Check out these resources to learn more:\n",
    "\n",
    "- https://www.coursera.org/learn/machine-learning\n",
    "- https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\n",
    "- https://scikit-learn.org/stable/unsupervised_learning.html\n",
    "- https://scikit-learn.org/stable/modules/clustering.html"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
